import os
import re
import pandas as pd
from glob import glob





parent_dir = "/Users/janzules/Roselab/ctDNA_11042024/data/human_binned_sequences"
bin_range = "125_155"
intersect_folder = "geneIntersect"
# or 
# intersect_folder = "nuclIntersect"


gene_intersect_dir = os.path.join(bin_dir, "bedtool_out")


gene_intersect_dir



def processing_bins(bin_dir, intersect_folder):
    # Collecting bed files from the current bins directory
    intersect_dir = os.path.join(bin_dir, "bedtools_out", intersect_folder)
    bed_files = glob(os.path.join(intersect_dir, "*_genesIntersect.bed"))

    df_list = []

    # TODO: remove this item
    Anumber_lengths = len(bed_files)
    print(f"This is the number of .bed files in the folder: {Anumber_lengths}")
    for bedfile in bed_files:

        # TODO: Remove the prints and remove the first sample_id
        #testing
        sample_id = os.path.basename(bedfile)#.replace("_geneIntersect.bed","")
        print(f"This is the befile: {bedfile}")
        print(f"This is the basename without replacement: {sample_id}")
        sample_id = os.path.basename(bedfile).replace("_genesIntersect.bed","")
        print(f"This is with the replacement: {sample_id}")
        print(" ")
        
   


bin_dir = os.path.join(parent_dir, bin_range)
processing_bins(bin_dir, intersect_folder)


bin_range
parent_dir
intersect_folder


test_file = os.path.join(parent_dir, bin_range,"bedtools_out", intersect_folder, "A14891_125_155_genesIntersect.bed")
print(test_file)


df_test = pd.read_csv(
    test_file,
    sep='\t',
    header=None,
    names=["chr","source","feature","start","end","score","strand","phase","attributes","count"]
    )

# subsetting for testing
df_sampled = df_test.sample(n=20, random_state=42)


df_test["count"].describe()


df_test["gene_id"] = df_test["attributes"].apply(
    lambda x: re.search(r"gene:([^;]+)", x).group(1) if re.search(r"gene:([^;]+)", x) else None
)
df_test["gene_name"] = df_test["attributes"].apply(
    lambda x: re.search(r"Name=([^;]+)", x).group(1) if re.search(r"Name=([^;]+)", x) else None
)


df_test.head()


df_group_counts = df_test.groupby(["gene_id", "gene_name"]).size().reset_index(name="count_occurrences")

# Filter only those with 2 or more occurrences
df_duplicates = df_group_counts[df_group_counts["count_occurrences"] >= 2]

# Display the duplicate groups
print(df_duplicates)



df_group_counts = df_test.groupby(["gene_id", "gene_name"]).size().reset_index(name="count_occurrences")

# Display the first few rows of the grouped counts
print(df_group_counts.head())



print(df_test["gene_id"].value_counts().head(20))  # Check the top 20 most frequent gene IDs












import os
import re
import pandas as pd
from glob import glob

def process_bin_gene_counts(bin_dir, intersect_folder):
    gene_intersect_dir = os.path.join(bin_dir, "bedtools_out", intersect_folder)
    
    bed_files = glob(os.path.join(gene_intersect_dir, "*_genesIntersect.bed"))
    df_list = []
    
    for bedfile in bed_files:
        # e.g. "A14891_125_155_genesIntersect.bed"
        sample_id = os.path.basename(bedfile).replace("_genesIntersect.bed","")  # "A14891_125_155"
        
        # Read the BED file
        # By default, pandas will treat # lines as comments if you have them. 
        # Adjust header=None if no header line in the file.
        print(f"Reading {sample_id}")
        df = pd.read_csv(
            bedfile,
            sep='\t',
            header=None,
            names=["chr","source","feature","start","end","score","strand","phase","attributes","count"]
        )
        
        # Parse gene_id and gene_name from the 'attributes' column
        # Look for strings like "ID=gene:ENSG00000186092;Name=OR4F5;..."
        # We'll capture "ENSG00000186092" and "OR4F5" with regex
        df["gene_id"] = df["attributes"].apply(
            lambda x: re.search(r"gene:([^;]+)", x).group(1) if re.search(r"gene:([^;]+)", x) else None
        )
        df["gene_name"] = df["attributes"].apply(
            lambda x: re.search(r"Name=([^;]+)", x).group(1) if re.search(r"Name=([^;]+)", x) else None
        )
        
        # Aggregate counts by gene
        df_sum = df.groupby(["gene_id","gene_name"], as_index=False)["count"].sum()
        
        # Attach sample info
        df_sum["sample"] = sample_id
        df_list.append(df_sum)

    print("Finished reading files")
    print("Combining.....")
    # Combine into one long df
    combined = pd.concat(df_list, ignore_index=True)
    print("Finished combining")
    # Pivot to wide format
    wide_counts = combined.pivot_table(
        index=["gene_id","gene_name"], 
        columns="sample", 
        values="count",
        fill_value=0
    ).reset_index()
    
    return wide_counts


# Example usage
# bin_dir = "/Users/.../ctDNA_11042024/data/human_binned_sequences/125_155"
# counts_125_155 = process_bin_gene_counts(bin_dir)
# counts_125_155.to_csv("125_155_gene_counts.tsv", sep="\t", index=False)

# If you want to do the loop for each bin
bins = ["125_155","150_180","170_220","220_345","40_150"]
parent = "/Users/janzules/Roselab/ctDNA_11042024/data/human_binned_sequences"
output_dir = "/Users/janzules/Roselab/ctDNA_11042024/data/human_binned_sequences/ctMatrices/"
#Which Counts?


intersect_type = 'gene'

if intersect_type == 'gene':
    intersect_folder = "geneIntersect"
    sub_folder = "geneCounts"
    out_name = "_gene_counts.tsv"
else:
    intersect_folder = "nuclIntersect"
    sub_folder = "nuclCounts"
    out_name = "_nucleosome_counts.tsv"

print(intersect_folder)



for b in bins:
    bin_path = os.path.join(parent, b)
    mat = process_bin_gene_counts(bin_path, intersect_folder)
    outname = os.path.join(output_dir, sub_folder, f"{b}{out_name}")
    mat.to_csv(outname, sep="\t", index=False)



